{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "ckktO-jNT1YZ",
        "6OyFhH8tTpah"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Imports"
      ],
      "metadata": {
        "id": "ckktO-jNT1YZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF35nsIGTJk9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Infrastructure"
      ],
      "metadata": {
        "id": "6OyFhH8tTpah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Markov Decision Process (MDP) class\n",
        "class MDP():\n",
        "\n",
        "\tdef __init__(self, states, actions, P, R, gamma):\n",
        "\t\tself.states = states      # List of all possible states\n",
        "\t\tself.actions = actions    # List of all possible actions\n",
        "\t\tself.P = P                # Transition probability function: P(s, a, s')\n",
        "\t\tself.R = R                # Reward function: R(s, a, s')\n",
        "\t\tself.gamma = gamma        # Discount factor (importance of future rewards)\n",
        "\n",
        "\n",
        "# Calculate the Q-value for a given state-action pair\n",
        "def Q_value(mdp, s, a, U):\n",
        "\treturn sum(mdp.P(s, a, state)*(mdp.R(s, a, state) + mdp.gamma*U[state]) for state in mdp.states)\n",
        "\n",
        "\n",
        "# Get the maximum difference between two sets of utilities\n",
        "def max_diff_utilities(U0, U1):\n",
        "\treturn max([abs(U0[s] - U1[s]) for s in U0])\n",
        "\n",
        "\n",
        "# Value Iteration algorithm to find optimal utility values\n",
        "def value_iteration(mdp, epsilon=0.01, max_iterations=100):\n",
        "\tU = {s: -100 for s in mdp.states}\n",
        "\tfor iteration_num in range(max_iterations):\n",
        "\t\tU_next = { state: max([Q_value(mdp, state, a, U) for a in mdp.actions]) for state in mdp.states }\n",
        "\t\tif max_diff_utilities(U, U_next) <= epsilon:\n",
        "\t\t\treturn U_next, iteration_num\n",
        "\t\tU = U_next\n",
        "\treturn U_next, iteration_num\n",
        "\n",
        "\n",
        "# Derive the best policy from utility values\n",
        "def policy_from_utility(mdp, U):\n",
        "\tpolicy = {}\n",
        "\tfor state in mdp.states:\n",
        "\t\tif all(Q_value(mdp, state, a, U) <= -100 for a in mdp.actions):\n",
        "\t\t\tpolicy[state] = 'none'\n",
        "\t\telse:\n",
        "\t\t\tbest_action = max(mdp.actions, key=lambda a: Q_value(mdp, state, a, U))\n",
        "\t\t\tpolicy[state] = best_action\n",
        "\treturn policy"
      ],
      "metadata": {
        "id": "g5mccypETfDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define the MDP"
      ],
      "metadata": {
        "id": "iR-8wlb0T6qL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transition probabilities for each state and action\n",
        "\n",
        "\n",
        "# Define the probability of transitioning from one state to another\n",
        "\n",
        "\n",
        "# Define the rewards for reaching different terminal states\n",
        "\n",
        "\n",
        "# Instantiate the MDP\n",
        "mdp = None"
      ],
      "metadata": {
        "id": "Fzm1u-ukT7GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Perform Value Iteration"
      ],
      "metadata": {
        "id": "vcLt9cMMUU9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Value Iteration\")\n",
        "print(\"----------------------\")\n",
        "U, i = value_iteration(mdp, 0.01)\n",
        "policy = policy_from_utility(mdp, U)\n",
        "print(f\"Iterations: {i}\")\n",
        "print(\"Utility:\")\n",
        "pprint(U)\n",
        "print(\"Policy:\")\n",
        "pprint(policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUiTv6U6UYSB",
        "outputId": "8e8c7126-eff9-43fe-8f40-022e978f9ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Iteration\n",
            "----------------------\n",
            "Iterations: 55\n",
            "Utility:\n",
            "{(0, 0): 19.913687108245117,\n",
            " (0, 1): 19.913687108245117,\n",
            " (0, 2): 19.913687108245117,\n",
            " (1, 0): 19.913687108245117,\n",
            " (1, 2): 19.913687108245117,\n",
            " (2, 0): 19.9136868862271,\n",
            " (2, 1): 19.91368616776171,\n",
            " (2, 2): 19.9136868862271,\n",
            " (3, 0): 19.913685951359735,\n",
            " (3, 1): 0.0,\n",
            " (3, 2): 0.0}\n",
            "Policy:\n",
            "{(0, 0): 'up',\n",
            " (0, 1): 'up',\n",
            " (0, 2): 'up',\n",
            " (1, 0): 'left',\n",
            " (1, 2): 'left',\n",
            " (2, 0): 'left',\n",
            " (2, 1): 'left',\n",
            " (2, 2): 'left',\n",
            " (3, 0): 'down',\n",
            " (3, 1): 'up',\n",
            " (3, 2): 'up'}\n"
          ]
        }
      ]
    }
  ]
}